\chapter{Implementation} \label{C:implementation} 

\todo[inline]{
    The final design of this project was ultimately the process of constant iteration. What is presented here is merely the design of the final implementation. 
}
This chapter discusses the details of the Kihi implementation presented in this report. The heart of this discussion is a description of the various performance driven designs decisions and features underlying the implementation. However, this chapter also aims to provide an overall understanding by discussing basic implementation details such as how the semantics outlined in chapter \ref{C:background} are implemented as code. But firstly, in order to guide the rest of the chapter, an overview of the implementation is presented below.

\section{Overview}
\input{figures/04_implementation/kihi_execution_process}
The implementation described in this project, which is also referred to as the Kihi Runner, is capable of directly handling Kihi source code. The execution process is outlined in figure \ref{fig:kihi_execution_process}, and as the figure shows, the source code is first parsed and transformed into an intermediary representation before execution. The details of the intermediary representation are discussed in section \ref{sec:implementation_intermediary_representation}. This intermediary is then optionally optimised through the process of symbol detection, and symbol optimisation which are the subjects of section \ref{sec:implementation_optimisation}. The details of the execution stage are omitted from the above figure due to desire for simplicity, but the process essentially involves selecting an executor, which can be understood as a virtual machine, and utilising it to execute the intermediary representation. The implementation supports three types of executors: a term rewriting based executor, a stack based executor, and a depreciated term rewriting based executor kept for posterity. The implementation details of the executor process and the executors are provided in section \ref{sec:implementation_executors}

\todo[inline]{
    maybe in background elaborate on what a virtual machine is?
}

\section{Configuration}
The Kihi Runner provides various configuration options available at compile time and run time. Compile time options are available through feature flags and run time options can be specified using command line flags. Feature flags make it possible to include or exclude various features such as the certain executors, CPU profiling, and output printing from the binary. This reduces the size of the binary and also removes unneeded features which might impact performance. This is especially true of output printing since formatting outputs incurs an noticeable runtime overhead. CPU profiling also incurs an, albeit minor, runtime overhead. The run time arguments are use to specify which executor to utilise, the degree of optimisation, and the number of outputs to print. More specific details of these configuration options are available in the project's readme.

\todo[inline]{
    move this:
    This intermediary representation is mostly identical to the grammar shown in figure \ref{fig:kihi_grammar} barring implementation details, debugging symbols, and optimisation mechanisms. The optimisation mechanisms are of particular interest and are discussed in greater detail in section \ref{sec:implementation_optimisation}.
    The executable takes as an argument the path to a text file contain Kihi source code, encoded in either UTF-8 or ASCII, and subsequently runs the program, emitting any outputs to standard output.
}

\section{Implementation Language}
This project was implemented in Rust. This decision was largely motivated by performance concerns and ease of use. Rust is considered a systems language and is designed with performance as a principal goal. An manifestation of this is manual control over memory in conjunction with a type system capable of guaranteeing memory safety. This provides the performance benefits of a low level language while retaining the safety benefits of a garbage collection. Furthermore, Rust also provides many features commonly found in functional programming languages, such as pattern matching, which significantly improve the ergonomics of the language especially in the context of programming language implementation as shown by the ubiquitous of such features throughout the codebase. Overall, these factors give Rust well earned respect as a language suitable for performance sensitive projects, and an especially compelling choice for this project.

\todo[inline]{Reference rust webpage?}

\section{Intermediary Representation}\label{sec:implementation_intermediary_representation}
The intermediary representation is essential identical to an abstract syntax tree of the grammar shown in figure \ref{fig:kihi_grammar} barring implementation details, debugging symbols, and optimisation mechanisms. The purpose of the intermediary representation is two fold. Firstly, it decouples the source code from the executable representation allowing room for the aforementioned differences. Secondly, it provides a common representation that can be interpreted by the various executors. An example of an intermediary representation is shown in figure \ref{fig:kihi_intermediary_representation_example_1}. The optimisation mechanisms introduced to the intermediary representation are the most interesting and are the subject of section \ref{sec:implementation_optimisation}. However, from the perspective of the intermediary representation they merely manifest as an additional term in the grammar denoted as `Symbol'.

\input{figures/04_implementation/kihi_intermediary_representation_example_1}


\section{Executors}\label{sec:implementation_executors}
Executors are used to compare the performance of various execution methods. They represent virtual machines that are capable of executing the intermediary representation and are essentially the implementations of the operational semantics shown in section \ref{sec:background_evaluation_methods}. In fact, an almost direct translation of the semantics presented in that section lies at the heart of this project's implementation. Furthermore, the algorithm used in this implementation is accurately represented by the pseudocode in figures \ref{fig:term_rewriting_pseudocode} and \ref{fig:stack_pseudocode}.

\subsection{Reduce Executor}
\input{figures/04_implementation/term_rewriting_pseudocode}

The heart of the reduce executor algorithm is relatively simple. The reduce executor algorithm consists of only two steps. The first step is finding a reducible term, this means finding an operator directly followed by a sufficient number of abstractions. This corresponds to the \lstinline{find_reductible_term} function shown in figure \ref{fig:term_rewriting_pseudocode}. Secondly, if an reducible term is found the corresponding reduction rule is applied, otherwise if no such terms can be found the program halts. This process is repeated until the program halts.

\subsection{Stack Executor}
\input{figures/04_implementation/stack_pseudocode}

The stack executor algorithm is equally simple or potentially even simpler. The algorithm consists of only a single step where the right most term in the program is inspected and the stack modified accordingly. For instance, when an abstraction is encountered it is pushed onto the stack, and when an apply is encountered an abstraction is popped off the stack and the abstraction's inner program appended to the terms. This is repeated until the right most term requires more arguments than there are values on the stack or the terms are exhausted.

Comparing the performance of executors is worthwhile because the implementations of the term rewriting semantics and the stack semantics, which hereinafter are referred to as the reduce executor and stack executor, are substantially different from a mechanical perspective. The reduce executor algorithm can be decomposed into two parts: reducible term finding and term reduction. In contrast, the stack executor algorithm simply consists of reduction.  Hypothetically it would appear that the stack executor would perform significantly better than the reduce executor simply due to the additional $\Theta(n)$ reducible term finding operation required before each reduction, as shown by line 4 of figure \ref{fig:term_rewriting_pseudocode}. Whether this holds in practice is the topic of chapter \ref{C:evaluation}.

As mentioned in earlier sections, a third executor also exists. This third executor is a previous implementation of the term rewriting operational semantics and is primarily kept for posterity. However, it also provides an useful indicator of how much the performance can vary when the underlying operational semantics remain the same.


\section{Optimisation}\label{sec:implementation_optimisation}
\input{figures/04_implementation/kihi_intermediary_representation_example_2}

\input{figures/04_implementation/kihi_symbol_optimisation}

The most significant contribution of this project is the creation of an approach for optimising Kihi programs. The approach essential involves replacing subprograms in Kihi with a potentially more efficient representation. This is accomplished in a two stage process. This is accomplished in two stage process. First, candidate subprograms are found through symbol detection and secondly these candidates are optimised using symbol optimisation. The subprograms are replaced by their corresponding symbols which can then executed using the representations created by the symbol optimisation process.

The symbol optimisation process essentially amounts to symbolic execution with placeholder inputs. The output of this execution provides an direct representation of the program's output parametric on the inputs. The symbolic execution is a limited form of stack execution where the `apply' operator can only be evaluated when the abstraction captures a statically known program. The output representation provides a template for the program's output given specific arguments. This allows for potentially faster execution since a simple find and replace procedure can be used to determine the output of executing a program given certain arguments rather than ordinary execution. Figure \ref{fig:kihi_symbol_optimisation} shows the output of this process for some small sample programs. 

\input{figures/04_implementation/symbol_optimisation_pseudocode}

The symbol detection process is designed to decompose the program into symbols that can be meaningfully optimised. There are many ways to decompose a program, however the algorithm utilised in the Kihi Runner is relatively simple. It simply combines any sequence of operators unbroken by abstractions into a single symbol. This is applied to the entire program and is recursively applied to the abstractions within programs. The output of this process when applied to the program in figure \ref{fig:kihi_example} is shown in figure \ref{fig:kihi_intermediary_representation_example_2}.

This overall strategy to optimisation is general and encourages further experimentation and research. The specific algorithms used for symbol detection and optimisation are not restricted to the ones utilised in this project. There are potentially more complex algorithms capable of handling more complex subprograms, this topic is discussed in further detail in chapter \ref{C:future_work}.

The origins of this approach are attempts at optimising the performance of the initial version of the Kihi Runner. In particular, they arose from initial attempts are optimising the Kihi Runner's performance on the count program presented in figure \ref{fig:kihi_example}. One such attempt was an symbol detection and symbol optimisation algorithm tailored to numbers. Although never completed, that algorithm would have been able to decode the church encoding of numbers and common operations such as addition. This would have allowed for a Kihi program to be efficient executed using basic assembly instructions, which are likely several magnitudes faster than ordinary execution. However, before this was fully developed it was generalised into the form that appears here.

\todo[inline]{Talk about why this might be faster than the ordinary execution}.

\todo[inline]{Talk about the actual design of the implementation}.

\todo[inline]{AST for the optimised representations}