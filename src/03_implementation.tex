\chapter{Design and Implementation} \label{C:implementation} 

This chapter discusses the design and implementation details of the Kihi Runner. This includes a description of the engineering aspects of the project, most notably the architecture. But the heart of this chapter is a description of the various performance-driven design decisions and features underlying the implementation. This chapter also aims to provide an overall understanding of the basic implementation details such as how the semantics outlined in chapter \ref{C:background} are implemented. This chapter also describes the extensive benchmark suite created for the project's evaluation. Firstly however, a brief overview of the Kihi Runner's features is given in order to guide the chapter.

\section{Overview}
\input{figures/04_implementation/kihi_execution_process}
The implementation, also referred to as the Kihi Runner, is a command-line binary executable capable of executing Kihi programs. The executable takes as input Kihi source code and outputs the result of the execution. Internally, the source is first parsed into an intermediary representation and subsequently optimised before actual execution. Figure \ref{fig:kihi_execution_process} illustrates this process visually. The details of the intermediary representation are given in section \ref{sec:implementation_intermediary_representation}. The optimisation process consists of two steps: symbol detection and symbol optimisation. The details of these two steps are the subjects of section \ref{sec:implementation_optimisation}. Furthermore, the Kihi Runner allows the user to specify the operational semantics of the execution process by specifying an executor using command-line flags. The Kihi Runner supports three executors by default: an executor using term rewriting semantics, an executor using stack semantics, and an obsolete term rewriting executor kept for posterity. The implementation details of these executors are given in section \ref{sec:implementation_executors}

\section{Architecture}
\input{figures/04_implementation/architecture}

The architecture of the application can be separated into four modules. These modules are the representation module, optimisation module, executor module, and the interface module. These modules provide an effective separation of concerns. The representation module is responsible for handling the representation of the source code internally and contains the definitions of the intermediary representation and parser. The executor module defines the three default executors and the executor interface, potentially allow for third-party executors to be seamlessly integrated. The optimisation module contains the various optimisers and the optimiser interface, which likewise potentially allows for third-party optimisers to be utilised. Finally, the interface module provides the command-line interface that users utilise to invoke the Kihi Runner.

\section{Configuration}
The Kihi Runner provides various configuration options available at compile time and run time. Compile time options are available through feature flags and run time options can be specified using command line flags. Feature flags make it possible to include or exclude various features such as certain executors, CPU profiling, and output printing from the binary. This reduces the size of the binary and also removes unneeded features which might impact performance. This is especially true of output printing since formatting outputs incurs a noticeable runtime overhead. CPU profiling also incurs an, albeit minor, runtime overhead. The run time arguments are used to specify which executor to utilise, the degree of optimisation, and the number of outputs to print. More specific details of these configuration options are available in the project's readme.

\section{Implementation Language}
This project was implemented in Rust. This decision was largely motivated by performance concerns and ease of use. Rust is considered a systems language and is designed with performance as a principal goal \cite{matsakis2014rust}. A manifestation of this is manual control over memory in conjunction with a type system capable of guaranteeing memory safety. This provides the performance benefits of a low-level language while retaining the safety benefits of a garbage collection. Furthermore, Rust also provides many features commonly found in functional programming languages, such as pattern matching, which significantly improve the ergonomics of the language especially in the context of programming language implementation as shown by the ubiquitous of such features throughout the codebase. Overall, these factors give Rust well earned respect as a language suitable for performance sensitive projects, and an especially compelling choice for this project.

\section{Intermediary Representation}\label{sec:implementation_intermediary_representation}
The intermediary representation is essentially identical to an abstract syntax tree of the grammar shown in figure \ref{fig:kihi_grammar} barring implementation details, debugging symbols, and optimisation mechanisms. The purpose of the intermediary representation is two-fold. Firstly, it decouples the source code from the executable representation allowing room for the aforementioned differences. Secondly, it provides a common representation that can be interpreted by the various executors. An example of an intermediary representation is shown in figure \ref{fig:kihi_intermediary_representation_example_1}. The optimisation mechanisms introduced to the intermediary representation are the most interesting and are the subject of section \ref{sec:implementation_optimisation}. However, from the perspective of the intermediary representation they merely manifest as an additional term in the grammar denoted as `Symbol'.

\input{figures/04_implementation/kihi_intermediary_representation_example_1}


\section{Executors}\label{sec:implementation_executors}
Executors are used to compare the performance of various execution methods. They represent virtual machines that are capable of executing the intermediary representation and are essentially the implementations of the operational semantics shown in section \ref{sec:background_evaluation_methods}. In fact, an almost direct translation of the semantics presented in that section lies at the heart of this project's implementation. Furthermore, the algorithm used in this implementation is accurately represented by the pseudocode in figures \ref{fig:term_rewriting_pseudocode} and \ref{fig:stack_pseudocode}.

\subsection{Reduce Executor}
\input{figures/04_implementation/term_rewriting_pseudocode}

The heart of the reduce executor algorithm is relatively simple. The reduce executor algorithm consists of only two steps. The first step is finding a reducible term, i.e. an operator directly followed by a sufficient number of abstractions. This corresponds to the \lstinline{find_reductible_term} function shown in figure \ref{fig:term_rewriting_pseudocode}. Secondly, once a reducible term is found the corresponding reduction rule is applied, and if no such terms can be found the program halts. This process is repeated until the program halts.

\subsection{Stack Executor}
\input{figures/04_implementation/stack_pseudocode}

The stack executor algorithm is equally simple or potentially even simpler. The algorithm consists of only a single step where the rightmost term in the program is inspected and the stack modified accordingly. For instance, when an abstraction is encountered it is pushed onto the stack, and when an apply is encountered an abstraction is popped off the stack and the abstraction's inner program appended to the terms. This is repeated until the rightmost term requires more arguments than there are values on the stack or the terms are exhausted.

Comparing the performance of executors is worthwhile because the implementations of the term rewriting semantics and the stack semantics, which hereinafter are referred to as the reduce executor and stack executor, are substantially different from a mechanical perspective. The reduce executor algorithm can be decomposed into two parts: reducible term finding and term reduction. In contrast, the stack executor algorithm simply consists of reduction.  Hypothetically it would appear that the stack executor would perform significantly better than the reduce executor simply due to the additional $\Theta(n)$ reducible term finding operation required before each reduction, as shown by line 4 of figure \ref{fig:term_rewriting_pseudocode}.

As mentioned in earlier sections, a third executor also exists. This third executor is a previous implementation of the term rewriting operational semantics and is primarily kept for posterity. However, it also provides an useful indicator of how much the performance can vary when the underlying operational semantics remain the same.


\section{Optimisation}\label{sec:implementation_optimisation}
\input{figures/04_implementation/kihi_intermediary_representation_example_2}

\input{figures/04_implementation/kihi_symbol_optimisation}

The most significant contribution of this project is the creation of an approach for optimising Kihi programs. The approach essential involves replacing subprograms in Kihi with a potentially more efficient representation. This is accomplished in a two-stage process. First, candidate subprograms are found through symbol detection and secondly these candidates are optimised using symbol optimisation. The subprograms are replaced by their corresponding symbols which can then executed using the representations created by the symbol optimisation process. Figure \ref{fig:kihi_symbol_optimisation} shows the output of this process for some small sample programs. 

The symbol optimisation process essentially amounts to symbolic execution with placeholder inputs. The output of this execution provides a direct representation of the program's output parametric on the inputs. The symbolic execution is a limited form of stack execution where the `apply' operator can only be evaluated when the abstraction captures a statically known program. The output representation provides a template for the program's output given specific arguments. This allows for potentially faster execution since a simple find and replace procedure can be used to determine the output of executing a program given certain arguments rather than ordinary execution. 

\input{figures/04_implementation/symbol_optimisation_pseudocode}

The symbol detection process is designed to decompose the program into symbols that can be meaningfully optimised. There are many ways to decompose a program, however the algorithm utilised in the Kihi Runner is relatively simple. It simply combines any sequence of operators unbroken by abstractions into a single symbol. This is applied to the entire program and is recursively applied to the abstractions within programs. The output of this process when applied to the program in figure \ref{fig:kihi_example} is shown in figure \ref{fig:kihi_intermediary_representation_example_2}.

This overall strategy to optimisation is generally applicable and encourages further experimentation and research. The specific algorithms used for symbol detection and optimisation are not restricted to the ones utilised in this project. There are potentially more complex algorithms capable of handling more complex subprograms, this topic is discussed in further detail in chapter \ref{C:work}.

The origins of this approach are attempts at optimising the performance of the initial version of the Kihi Runner. In particular, they arose from initial attempts at optimising the Kihi Runner's performance on the count program presented in figure \ref{fig:kihi_example}. One such attempt was a symbol detection and symbol optimisation algorithm tailored to numbers. Although never completed, the algorithm would have been able to identify church numerals and common operations such as addition. This would have allowed for a Kihi program with such terms to be executed using extremely fast native instructions. However, before this was fully developed it was generalised into the form that appears here.


\section{Benchmark Suite}
The implementation also includes two benchmark suites: a micro benchmark suite and a macro benchmark suite. The micro benchmark suite examines the performance of individual components of the implementation. It includes benchmarks of the parser, optimisers, basic operators, and symbol optimised operators each with a wide range of inputs. It was implemented using Rust and the Criterion library. This suite also performs statistical analysis on the results and can identify performance regressions. However, this suite is limited to measuring time elapsed. The macro benchmark suite executes entire programs with all possible combinations of executors and optimisation levels. This suite was implemented using bash scripting and the `perf' command-line tool. This benchmark suite also includes statistical analysis and measures elapsed time, but likewise includes low-level information such as CPU instructions, branch mispredictions and cache misses. Together these benchmark suites provide a convenient tool for performing a holistic evaluation of the Kihi Runner.